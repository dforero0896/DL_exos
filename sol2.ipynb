{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import dlc_practical_prologue as prologue\n",
    "os.environ[\"PYTORCH_DATA_DIR\"]=\"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]* Using MNIST\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "9920512it [00:05, 1970789.59it/s]                             \n",
      "Extracting data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
      "0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "32768it [00:00, 83657.64it/s]            \n",
      "0it [00:00, ?it/s]Extracting data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "1654784it [00:00, 1898918.09it/s]                            \n",
      "0it [00:00, ?it/s]Extracting data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "8192it [00:00, 18010.24it/s]            \n",
      "Extracting data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = prologue.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "def nearest_classification(train_input, train_target, x):\n",
    "    distances = (train_input - x[None,:]).pow(2).mean(axis=-1)\n",
    "    return train_target[torch.argmin(distances)]\n",
    "\n",
    "\n",
    "print(nearest_classification(train_input, train_target, test_input[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(train_input, train_target, test_input, test_target, mean=None, proj=None):\n",
    "    ntest = test_input.shape[0]\n",
    "    prediction = torch.empty(ntest)\n",
    "    accuracy = torch.empty(ntest)\n",
    "    if mean is not None:\n",
    "        train_input-=mean\n",
    "        test_input-=mean\n",
    "    if proj is not None:\n",
    "        train_input=train_input.mm(proj.t())\n",
    "        test_input=test_input.mm(proj.t())\n",
    "    for i in range(ntest):\n",
    "        prediction[i] = nearest_classification(train_input, train_target, test_input[i])\n",
    "    accuracy = prediction!=test_target\n",
    "    return accuracy.int().sum(), (accuracy).float().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(746), tensor(0.7460))"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "compute_nb_errors(train_input, train_target, test_input, test_target, mean=None, proj = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PCA(x):\n",
    "    \n",
    "    mean = x.mean(axis=0)\n",
    "    std_x = x - mean\n",
    "    cov = std_x.t().mm(std_x)\n",
    "    eival, eivect = torch.eig(cov, eigenvectors=True)\n",
    "    _,  sorter = torch.sort(eival[:,0], descending=True)\n",
    "    return mean, eivect[:,sorter].t()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([784, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "mean, eibase = PCA(train_input)\n",
    "eibase.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n",
      "Original errors: nb_errors 172 error 17.20%\n",
      "Random basis: nb_errors 210 error 21.00%\n",
      "PCA 100d nb_errors 164 error 16.40%\n",
      "PCA 50d nb_errors 155 error 15.50%\n",
      "PCA 10d nb_errors 214 error 21.40%\n",
      "PCA 3d nb_errors 597 error 59.70%\n",
      "* Using CIFAR\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n",
      "Original errors: nb_errors 746 error 74.60%\n",
      "Random basis: nb_errors 776 error 77.60%\n",
      "PCA 100d nb_errors 745 error 74.50%\n",
      "PCA 50d nb_errors 737 error 73.70%\n",
      "PCA 10d nb_errors 763 error 76.30%\n",
      "PCA 3d nb_errors 839 error 83.90%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "for c in [ False, True ]:\n",
    "\n",
    "    train_input, train_target, test_input, test_target = prologue.load_data(cifar = c)\n",
    "\n",
    "    # Original test on dataset. Define baseline\n",
    "\n",
    "    nb_errors, perc_errors = compute_nb_errors(train_input, train_target, test_input, test_target)\n",
    "    print(f'Original errors: nb_errors {nb_errors} error {100*perc_errors:.02f}%')\n",
    "\n",
    "    # Project onto 100d random basis, that is a set of d 100-elem long vectors. \n",
    "    # the basis defines the projection matrix.\n",
    "\n",
    "    basis = train_input.new(100, train_input.size(1)).normal_()\n",
    "\n",
    "    nb_errors, perc_errors = compute_nb_errors(train_input, train_target, test_input, test_target, None, basis)\n",
    "    print(f'Random basis: nb_errors {nb_errors} error {100*perc_errors:.02f}%')\n",
    "\n",
    "    # Project onto PCA space with the first npc PCs taken into account\n",
    "\n",
    "    mean, basis = PCA(train_input)\n",
    "    \n",
    "    for npc in [ 100, 50, 10, 3 ]:\n",
    "        nb_errors, perc_errors = compute_nb_errors(train_input, train_target, test_input, test_target, mean, basis[:npc])\n",
    "        print(f'PCA {npc:d}d nb_errors {nb_errors} error {100*perc_errors:.02f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}